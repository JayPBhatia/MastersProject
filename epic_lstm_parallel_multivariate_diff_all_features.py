# -*- coding: utf-8 -*-
"""EPIC_LSTM_Parallel_Multivariate_diff_all_features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z9n9fpkfWMT8_7Srh-AePoBpB9Ut6Agn

## Setup
"""

#from google.colab import drive
#drive.mount('/content/gdrive/')
#path = 'gdrive/MyDrive/ColabOutput/'

path = './'

# Commented out IPython magic to ensure Python compatibility.
import math
import os
from itertools import product

import numpy as np
from numpy import array

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import scipy.stats as scs

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn import preprocessing

import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
from statsmodels.graphics.tsaplots import plot_acf , plot_pacf
from statsmodels.tsa.stattools import acf, pacf, grangercausalitytests, adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.statespace.varmax import VARMAX


import joblib

import tensorflow as tf
from tqdm.keras import TqdmCallback
from tqdm import tqdm_notebook


import warnings
warnings.filterwarnings('ignore')


# %matplotlib inline
plt.rcParams['figure.figsize'] = [10, 7.5]

seed = 7
np.random.seed(seed)

"""### Healper Functions

## Dataset
"""

all_19_oct_data = []

url_prefix = "https://raw.githubusercontent.com/JayPBhatia/MastersProject/main/data/itrust/"
# 
epic_19_oct_data_files = ["EpicLog_Scenario_1_19_Oct_2018_14_44.csv", 
                   "EpicLog_Scenario_2_19_Oct_2018_14_56.csv", 
                   "EpicLog_Scenario_3_19_Oct_2018_15_02.csv", 
                   "EpicLog_Scenario_4_19_Oct_2018_15_23.csv", 
                   "EpicLog_Scenario_5_19_Oct_2018_15_45.csv", 
                   "EpicLog_Scenario_6_19_Oct_2018_16_06.csv"]

for filename in epic_19_oct_data_files:
    file_df = pd.read_csv(url_prefix+filename, index_col=None, header=0)
    all_19_oct_data.append(file_df)

df = pd.concat(all_19_oct_data, axis=0, ignore_index=True)
df['Timestamp'] = pd.to_datetime(df['Timestamp'], infer_datetime_format=True)
df = df.drop_duplicates(subset='Timestamp', keep="last")
df = df.set_index('Timestamp')
df = df.filter(regex='Generation.GIED1.Measurement')

df_org = df.copy()
df = df.diff().dropna().reset_index(drop=True)

def add_rolling_features(df_out_concat, cols):
  for col_for_mean in cols:
    df_out_concat[col_for_mean +'.Rolling.Mean2' ] = df_out_concat[col_for_mean ].rolling(window=2).mean()
    df_out_concat[col_for_mean +'.Rolling.Mean2.diff' ] = df_out_concat[col_for_mean ] - df_out_concat[col_for_mean +'.Rolling.Mean2' ] 

    df_out_concat[col_for_mean +'.Rolling.Mean5' ] = df_out_concat[col_for_mean ].rolling(window=5).mean()
    df_out_concat[col_for_mean +'.Rolling.Mean5.diff' ] = df_out_concat[col_for_mean ] - df_out_concat[col_for_mean +'.Rolling.Mean5' ] 

    df_out_concat[col_for_mean +'.Rolling.Mean10'] = df_out_concat[col_for_mean ].rolling(window=10).mean()
    df_out_concat[col_for_mean +'.Rolling.Mean10.diff' ] = df_out_concat[col_for_mean ] - df_out_concat[col_for_mean +'.Rolling.Mean10' ] 


def add_features(df_in):

  df_in_diff1 =  df_in.copy().diff().reset_index(drop=True)

  df_in_shift1 = df_in.copy().shift(1).reset_index(drop=True)
  df_in_shift1 = df_in_shift1.add_suffix(".shift1")

  df_in_shift1_firstOrderDiff = df_in_shift1.diff().reset_index(drop=True)
  df_in_shift1_firstOrderDiff = df_in_shift1_firstOrderDiff.add_suffix(".firstOrderDiff")

  df_in_shift1_secondOrderDiff = df_in_shift1_firstOrderDiff.diff().reset_index(drop=True)
  df_in_shift1_secondOrderDiff = df_in_shift1_secondOrderDiff.add_suffix(".secondOrderDiff")

  df_in_shift1_diff2 = df_in_shift1.diff(2).reset_index(drop=True)
  df_in_shift1_diff2 = df_in_shift1_diff2.add_suffix(".diff2")

  df_in_shift1_diff5 = df_in_shift1.diff(5).reset_index(drop=True)
  df_in_shift1_diff5 = df_in_shift1_diff5.add_suffix(".diff5")

  df_in_shift1_diff10 = df_in_shift1.diff(10).reset_index(drop=True)
  df_in_shift1_diff10 = df_in_shift1_diff10.add_suffix(".diff10")

  df_out_concat = pd.concat([df_in_shift1_firstOrderDiff, 
                             df_in_shift1_secondOrderDiff, 
                             df_in_shift1_diff2,
                             df_in_shift1_diff5,
                             df_in_shift1_diff10], axis=1)

  add_rolling_features(df_out_concat, df_in_shift1_firstOrderDiff.columns)
  add_rolling_features(df_out_concat, df_in_shift1_secondOrderDiff.columns)
  add_rolling_features(df_out_concat, df_in_shift1_diff2.columns)
  add_rolling_features(df_out_concat, df_in_shift1_diff5.columns)
  add_rolling_features(df_out_concat, df_in_shift1_diff10.columns)

  df_out_concat = pd.concat([df_out_concat, df_in_diff1], axis=1)

  return  df_out_concat

#df_new = df.copy().reset_index(drop=True)
df_new = add_features(df)
df_new = df_new.dropna().reset_index(drop=True)

print("*****************************************    training model for :  ********************************************")
n = len(df_new)
train_df = df_new[0:int(n*0.85)].copy()
val_df = df_new[int(n*0.85):int(n*0.95)].copy()
test_df = df_new[int(n*0.95):].copy()

print("train_df shape : ", train_df.shape)
print("val_df shape : ", val_df.shape)
print("test_df shape : ", test_df.shape)

x_scaler = preprocessing.MinMaxScaler()
y_scaler = preprocessing.MinMaxScaler()

all_cols = train_df.columns 

x_scaler.fit(train_df[all_cols[ :-1*df.columns.size]])
y_scaler.fit(train_df[all_cols[-1*df.columns.size:]])


x_scaler_filename = path + "x_scaler.save"
joblib.dump(x_scaler, x_scaler_filename) 

y_scaler_filename = path + "y_scaler.save"
joblib.dump(y_scaler, y_scaler_filename)

train_df_scaled = train_df.copy().reset_index(drop=True)
train_df_scaled[all_cols[ :-1*df.columns.size]] = pd.DataFrame(x_scaler.transform(train_df[all_cols[ :-1*df.columns.size]]))
train_df_scaled[all_cols[-1*df.columns.size:]] = pd.DataFrame(y_scaler.transform(train_df[all_cols[-1*df.columns.size:]]))
train_df_scaled = train_df_scaled.dropna()
print("train_df_scaled shape : ", train_df_scaled.shape)


val_df_scaled = val_df.copy().reset_index(drop=True)
val_df_scaled[all_cols[ :-1*df.columns.size]] = pd.DataFrame(x_scaler.transform(val_df[all_cols[ :-1*df.columns.size]]))
val_df_scaled[all_cols[-1*df.columns.size:]] = pd.DataFrame(y_scaler.transform(val_df[all_cols[-1*df.columns.size:]]))
val_df_scaled = val_df_scaled.dropna()
print("val_df_scaled shape : ", val_df_scaled.shape)

test_df_scaled = test_df.copy().reset_index(drop=True)
test_df_scaled[all_cols[ :-1*df.columns.size]] = pd.DataFrame(x_scaler.transform(test_df[all_cols[ :-1*df.columns.size]]))
test_df_scaled[all_cols[-1*df.columns.size:]] = pd.DataFrame(y_scaler.transform(test_df[all_cols[-1*df.columns.size:]]))
test_df_scaled = test_df_scaled.dropna()
print("test_df_scaled shape : ", test_df_scaled.shape)

n_steps = 16
def split_sequences(sequences, n_steps, m_cols):
	X, y = list(), list()
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps +1
		# check if we are beyond the dataset
		if end_ix > len(sequences):
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix-1, :-1*m_cols], sequences[end_ix-1, -1*m_cols:]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
X_train, y_train = split_sequences(train_df_scaled.values, n_steps, df.columns.size)
X_val, y_val = split_sequences(val_df_scaled.values, n_steps, df.columns.size)
X_test, y_test = split_sequences(test_df_scaled.values, n_steps, df.columns.size)

print("X_train shape: ", X_train.shape, " y_train shape: ", y_train.shape)
print("X_val shape: ", X_val.shape, " y_val shape: ", y_val.shape)
print("X_test shape: ", X_test.shape, " y_test shape: ", y_test.shape)

# Prepare the training dataset
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=64).batch(256)

# Prepare the validation dataset
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
val_dataset = val_dataset.batch(64)

lstm_model = tf.keras.models.Sequential()
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.3, return_sequences=True)))
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.2, return_sequences=True)))
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.1, return_sequences=True)))
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.05, return_sequences=True)))
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.025, return_sequences=True)))
lstm_model.add(tf.keras.layers.Bidirectional( tf.keras.layers.LSTM(8, input_shape=(X_train.shape[1], X_train.shape[2]))))
lstm_model.add(tf.keras.layers.Dense(y_train.shape[1]))
lstm_model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.MeanSquaredError(), #loss=tf.keras.losses.Huber(delta=1.35), 
              metrics=["mae", "mse", "msle", "logcosh", tf.keras.metrics.RootMeanSquaredError()])
early_stoppong_callback = tf.keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=64, restore_best_weights=True)
tqdm_callback = TqdmCallback(verbose=1)
history = lstm_model.fit(train_dataset, validation_data=val_dataset, epochs=1024, validation_steps=4, callbacks=[early_stoppong_callback, tqdm_callback ], verbose=0)
lstm_model.save(path + 'epic_lstm_model.h5')

def plot_history(history, key):
  plt.plot(history.history[key][20:])
  plt.plot(history.history['val_'+key][20:])
  plt.xlabel("Epochs")
  plt.ylabel(key)
  plt.legend([key, 'val_'+key])
  plt.show()
plt.figure(figsize=(20,5))
plot_history(history, 'mse')
plt.figure(figsize=(20,5))
plot_history(history, 'mae')

y_pred=lstm_model.predict(X_test)
y_test_inv = pd.DataFrame(y_scaler.inverse_transform(y_test))
y_pred_inv = pd.DataFrame(y_scaler.inverse_transform(y_pred))

for i, col in enumerate(df.columns):

  h = tf.keras.losses.Huber()
  print(col + " huber loss : ", h(y_test_inv[i], y_pred_inv[i]).numpy())
  print(col + " std : ", df[col].describe().loc[['std']][0])

  plt.figure(figsize=(30,5))
  plt.plot(df_org[col], color='blue')
  plt.show()

  plt.figure(figsize=(30,5))
  plt.plot(y_pred_inv[i], color='red')
  plt.plot(y_test_inv[i], color='green')
  plt.show()

  plt.figure(figsize=(30,5))
  plt.plot(abs(y_pred_inv[i] - y_test_inv[i]), color='black')
  plt.show()

url_prefix = "https://raw.githubusercontent.com/JayPBhatia/MastersProject/main/data/itrust/"
# 
epic_data_files = ["EpicLog_Scenario_7_07_Nov_2018_14_40.csv", 
                   "EpicLog_Scenario_8_07_Nov_2018_14_57.csv"]

for epic_file in epic_data_files:
  print("*****************************************    evaluate model for : " , epic_file , "  ********************************************")
  # move label col to last 
  df_eval = pd.read_csv(url_prefix+epic_file)
  df_eval = df_eval.filter(regex='Generation.GIED1.Measurement')
  print("df_eval.shape ", df_eval.shape)

  df_eval_new = add_features(df_eval)
  print("df_eval_new.shape ", df_eval_new.shape)
  df_eval_new = df_eval_new.dropna()

  new_x_scaler = joblib.load(path + 'x_scaler.save')
  new_y_scaler = joblib.load(path + 'y_scaler.save')
  new_lstm_model = tf.keras.models.load_model(path + 'epic_lstm_model.h5')

  eval_cols = df_eval_new.columns 

  df_eval_scaled = df_eval_new.copy()  
  df_eval_scaled[eval_cols[ :-1*df.columns.size]] = pd.DataFrame(new_x_scaler.transform(df_eval_new[eval_cols[ :-1*df.columns.size]]))
  df_eval_scaled[eval_cols[-1*df.columns.size:]] = pd.DataFrame(new_y_scaler.transform(df_eval_new[eval_cols[-1*df.columns.size:]]))

  X_eval, y_eval = split_sequences(df_eval_scaled.values, n_steps, df.columns.size)
  print("X_eval.shape ", X_eval.shape)
  print("y_eval.shape ", y_eval.shape)

  result = new_lstm_model.evaluate(X_eval, y_eval, verbose=1)
  print("result  : " , result)

  y_eval_pred = new_lstm_model.predict(X_eval)
  y_eval_inv = pd.DataFrame(new_y_scaler.inverse_transform(y_eval))
  y_eval_pred_inv = pd.DataFrame(new_y_scaler.inverse_transform(y_eval_pred))



  for i, col in enumerate(df.columns):

    h = tf.keras.losses.Huber()
    print(col + " huber loss : ", h(y_eval_inv[i], y_eval_pred_inv[i]).numpy())
    print(col + " std : ", df[col].describe().loc[['std']][0])

    plt.figure(figsize=(30,5))
    plt.plot(df_eval[col], color='blue')
    plt.show()


    plt.figure(figsize=(30,5))
    plt.plot(y_eval_pred_inv[i], color='red')
    plt.plot(y_eval_inv[i], color='green')
    plt.show()

    plt.figure(figsize=(30,5))
    plt.plot(abs(y_eval_pred_inv[i] - y_eval_inv[i]), color='black')
    plt.show()